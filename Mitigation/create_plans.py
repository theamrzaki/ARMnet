import json
import csv
from time import sleep
import torch
import utils
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from transformers import BitsAndBytesConfig
from huggingface_hub import login
login("secret_token_here")  # replace with your HF token if needed

# ===========================
# Config
# ===========================
PLAN_DIR = "output/Plans"  # plans generated by the trained model
ROOT_CAUSE_JSON = "adservice_cpu_1_baro_re1_ob.json"  # JSON with list of top 10 root causes
OUTPUT_CSV = "llm_judged_evaluation"



TRAINED_MODELS = [
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility_safety",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility_safety_scalability",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility_safety_scalability_efficiency",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility_safety_scalability_efficiency_effectiveness",
    "/home/db2003/Desktop/Amr/Tests/RCAEval/saved_models/custom_completeness_clarity_feasibility_safety_scalability_efficiency_effectiveness_compliance",
]

# Judge models (used to score each criterion)
JUDGE_MODELS = [
    "meta-llama/Llama-3.2-3B-Instruct",
    "ibm-granite/granite-4.0-micro",            # strong general instruction-following
    "microsoft/bitnet-b1.58-2B-4T",  # well-rounded chat-style outputs
    "Qwen/Qwen3-1.7B",                        # strong general instruction-following
    #"deepseek-ai/deepseek-coder-1.3b-instruct",              # smaller but fast, practical for local testing
    "google/gemma-2-2b-it"
]


CRITERIA = {
    "response in a well-structured JSON format": (0, 10),
    "mitigation plan can be implemented, as that it is feasible": (0, 10),
    "mitigation plan is efficient, as that it solves the root cause": (0, 10),
    "mitigation plan is scalable, as that it can be applied for large-scale issues": (0, 10),
    "mitigation plan is clear for the operator, as that it provides detailed steps and guidance": (0, 10),
}


import gc
import torch

import gc
import torch

def free_gpu_memory(obj=None):
    """Safely and completely release GPU memory used by a model/pipeline."""
    if obj is not None:
        try:
            if hasattr(obj, "model"):
                del obj.model
            if hasattr(obj, "tokenizer"):
                del obj.tokenizer
        except Exception:
            pass
        del obj

    gc.collect()                           # collect unused Python objects
    torch.cuda.empty_cache()               # clear the allocator cache
    torch.cuda.ipc_collect()               # clear interprocess memory references
    torch.cuda.synchronize()               # ensure pending operations finish



# ===========================
# Initialize judge generator (sequential loading)
# ===========================
def init_pipeline(model_name):
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
    import torch

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    if "bitnet" in model_name.lower():
        print(f"[INFO] Detected BitNet model: {model_name} — skipping BitsAndBytes quantization.")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True
        )
    else:
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            llm_int8_enable_fp32_cpu_offload=True
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quant_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True
        )

    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer
    )

    return generator




# ===========================
# Build prompt per criterion
# ===========================
filtered_plans = {} 

import json
import re
import torch
import os

def build_mitigation_plan(trained_model_name, generator, root_cause_json=ROOT_CAUSE_JSON):
    mitigation_format = {
        "root_cause_problem_1":{
            "name": "Short name of the root cause problem",
            "description": "Detailed description of the root cause problem",
            "mitigation_steps": [
                "Step 1: Description of the first mitigation step",
                "Step 2: Description of the second mitigation step",
                "...",
                "Step N: Description of the Nth mitigation step"
            ],
            "expected_outcome": "Description of the expected outcome after implementing the mitigation steps",
        },
        "root_cause_problem_2":{
            "name": "Short name of the root cause problem",
            "description": "Detailed description of the root cause problem",
            "mitigation_steps": [
                "Step 1: Description of the first mitigation step",
                "Step 2: Description of the second mitigation step",
                "...",
                "Step N: Description of the Nth mitigation step"
            ],
            "expected_outcome": "Description of the expected outcome after implementing the mitigation steps",
        }
    }
    prompt = (
        "You are an expert system reliability engineer. Given a root cause of a system failure, "
        "propose a detailed mitigation plan to address the root cause and prevent future occurrences. "
        "Be specific and actionable in your recommendations, following best practices, and return the result strictly as JSON.\n\n"
        "format should be like:\n" + json.dumps(mitigation_format, indent=2) + "\n\n"
        "Root Causes:\n"
    )

    # Load root causes
    with open(root_cause_json, "r") as f:
        top_causes = json.load(f)
        top_causes = top_causes["0"][:3]  # limit to top 3
        for i, cause in enumerate(top_causes, 1):
            prompt += f"{i}. {cause}\n"
    prompt += "\nMitigation Plan:\n"

    # Generate model output (try 3 times, each with different seed)
    for seed in [1,2,3,4,5,6,7,8,9,10]:
        utils.set_seed(seed)
        outputs = generator(prompt, max_new_tokens=500, do_sample=True, temperature=0.7)
        full_text = outputs[0]["generated_text"]

        # === STEP 1: Extract the JSON part ===
        json_match = re.search(r"\{[\s\S]*\}", full_text)
        if json_match:
            json_text = json_match.group(0)
        else:
            print(f"[WARN] No JSON found for {trained_model_name}. Saving raw output.")
            json_text = full_text

        # === STEP 2: Try to parse JSON safely ===
        try:
            parsed_json = json.loads(json_text)
        except json.JSONDecodeError:
            print(f"[WARN] Invalid JSON structure for {trained_model_name}. Attempting to clean.")
            # Fix common issues like trailing commas, missing quotes, etc.
            json_text_clean = re.sub(r",\s*}", "}", json_text)
            json_text_clean = re.sub(r",\s*]", "]", json_text_clean)
            try:
                parsed_json = json.loads(json_text_clean)
            except Exception as e:
                parsed_json = {"raw_text": json_text}
                print(f"[ERROR] Could not parse JSON for {trained_model_name}: {e}")

        # === STEP 3: Organize and save output ===
        filtered_plan = {
            "model": trained_model_name,
            "plan": parsed_json
        }

        os.makedirs("output/Plans", exist_ok=True)
        output_path = f"output/Plans/{trained_model_name}/plan_seed_{seed}.json"
        # if not exist create directory
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w") as f:
            json.dump(filtered_plan, f, indent=2)
        print(f"✅ Saved clean mitigation plan to {output_path}")
    torch.cuda.empty_cache()
    return filtered_plan


def build_prompt(plan_text, criterion, min_score, max_score):
    return (
        f"Here is a mitigation plan:\n{plan_text}\n\n"
        f"On a scale from {min_score} being low to {max_score} being high, rate that the {criterion}. "
        f"Provide only a single numeric score."
    )


# ===========================
# Evaluate plan sequentially (per judge)
# ===========================
def evaluate_with_judges_sequential(plan_info, criteria, judge_model_names):
    results = {criterion: [] for criterion in criteria}

    # Evaluate each criterion using all judge models
    for model_name in judge_model_names:
        generator = init_pipeline(model_name)

        for criterion, (min_score, max_score) in criteria.items():

            raw_plan = plan_info["plan"]

            # If it has a 'raw_text' field, parse it
            if "raw_text" in raw_plan:
                try:
                    nested_plan = json.loads(raw_plan["raw_text"])
                except:
                    nested_plan = raw_plan["raw_text"]
            else:
                nested_plan = raw_plan

            # Convert to nicely formatted string for prompt
            plan_text_for_judge = json.dumps(nested_plan, indent=2)
            prompt = build_prompt(plan_text_for_judge, criterion, min_score, max_score)
            output = generator(prompt, max_new_tokens=50, do_sample=True, temperature=0.7)
            # extract numeric score
            output_text = output[0]["generated_text"]
            match = re.findall(r"[-+]?\d*\.\d+|\d+", output_text)
            if match:
                score = float(match[-1])  # take the last number
            else:
                score = None
            if score > max_score:
                score = max_score
            if score < min_score:
                score = min_score
            results[criterion].append(score)

        free_gpu_memory(generator)
        sleep(15)

    # compute averages
    avg_scores = {criterion: sum(scores)/len(scores) for criterion, scores in results.items()}
    return results, avg_scores


# ===========================
# Generate and Evaluate all plans
# ===========================
print("Generating mitigation plans...")
for trained_model_name in TRAINED_MODELS:
    generator = init_pipeline(trained_model_name)
    trained_model_name = trained_model_name.replace("/", "_")
    filtered_plans[trained_model_name] = build_mitigation_plan(trained_model_name, generator)
    if hasattr(generator, "model"):
        del generator.model
    if hasattr(generator, "tokenizer"):
        del generator.tokenizer
    free_gpu_memory(generator)
    print("sleeping to clear memory...")
    sleep(20)  # brief pause to ensure memory is cleared
    print("waking up...")

print("===========================")
print("Evaluating mitigation plans with judges...")
print("===========================")

# stop the program here for now
exit(0)


# ===========================
# Main evaluation loop (fixed)
# ===========================
csv_rows = []

for trained_model_name in tqdm(TRAINED_MODELS):
    trained_model_name_clean = trained_model_name.replace("/", "_")
    plan_info = filtered_plans[trained_model_name_clean]
    plan_text = json.dumps(plan_info["plan"], indent=2)

    per_judge_scores, avg_scores = evaluate_with_judges_sequential(plan_info, CRITERIA, JUDGE_MODELS)

    row = {
        "trained_model": trained_model_name_clean,
        "plan": plan_text
    }

    # Add per-judge scores correctly
    for criterion, scores in per_judge_scores.items():
        for i, score in enumerate(scores):
            row[f"{criterion}_judge{i+1}"] = score

    # Add average scores per criterion
    for criterion, avg in avg_scores.items():
        row[f"{criterion}_avg"] = avg

    # Compute overall average
    row["overall_avg"] = sum(avg_scores.values()) / len(avg_scores)
    csv_rows.append(row)

# ===========================
# Save CSV
# ===========================
fieldnames = ["trained_model"] + \
            [f"{c}_judge{i+1}" for c in CRITERIA for i in range(len(JUDGE_MODELS))] + \
            [f"{c}_avg" for c in CRITERIA] + ["overall_avg"]

with open(OUTPUT_CSV+".csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    writer.writeheader()
    for row in csv_rows:
        del row["plan"]
    writer.writerows(csv_rows)

print(f"✅ LLM judged evaluation saved to {OUTPUT_CSV}")

